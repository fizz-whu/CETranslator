import Foundation
import Speech
import AVFoundation
import SwiftUI
import Translation

final class SpeechTranslationViewModel: ObservableObject {
    // MARK: - Speech Recognition
    private let audioEngine = AVAudioEngine()
    private var recognitionTask: SFSpeechRecognitionTask?
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    // Add recognizers for supported languages
    private var speechRecognizerEN: SFSpeechRecognizer?
    private var speechRecognizerZH: SFSpeechRecognizer?
    private var speechRecognizerJA: SFSpeechRecognizer?
    private var speechRecognizerES: SFSpeechRecognizer?
    private var speechRecognizerIT: SFSpeechRecognizer? // Added IT recognizer
    private var speechRecognizerKO: SFSpeechRecognizer? // Added KO recognizer
    private var speechRecognizerFR: SFSpeechRecognizer? // Added FR recognizer
    private var speechRecognizerPT: SFSpeechRecognizer? // Added PT recognizer

    @Published var recognizedText: String = ""
    @Published var isRecording: Bool = false
    @Published var errorMessage: String?

    init() {
        // Configure speech recognizers
        speechRecognizerEN = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))
        speechRecognizerZH = SFSpeechRecognizer(locale: Locale(identifier: "zh-Hans"))
        speechRecognizerJA = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP"))
        speechRecognizerES = SFSpeechRecognizer(locale: Locale(identifier: "es-ES"))
        speechRecognizerIT = SFSpeechRecognizer(locale: Locale(identifier: "it-IT"))   // Initialize IT recognizer
        speechRecognizerKO = SFSpeechRecognizer(locale: Locale(identifier: "ko-KR"))   // Initialize KO recognizer
        speechRecognizerFR = SFSpeechRecognizer(locale: Locale(identifier: "fr-FR"))   // Initialize FR recognizer
        speechRecognizerPT = SFSpeechRecognizer(locale: Locale(identifier: "pt-PT"))   // Initialize PT recognizer
        requestSpeechPermissions()
    }

    private func requestSpeechPermissions() {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async {
                if status != .authorized {
                    self.errorMessage = NSLocalizedString("speech_recognition_not_authorized", 
                        comment: "Please enable microphone access in Settings to use voice recognition")
                }
            }
        }
        AVAudioSession.sharedInstance().requestRecordPermission { _ in }
    }

    private func getLocalizedErrorMessage(for errorType: String, language: String) -> String {
        switch errorType {
        case "no_speech_detected":
            switch language {
            case "zh-Hans":
                return "è®©æˆ‘ä»¬å†è¯•ä¸€æ¬¡ï¼è¯·æŒ‰ä½éº¦å…‹é£Žå¹¶é è¿‘è®¾å¤‡è¯´è¯ ðŸŽ¤"
            case "ja-JP":
                return "ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼ãƒžã‚¤ã‚¯ã‚’é•·æŠ¼ã—ã—ã¦ã€ãƒ‡ãƒã‚¤ã‚¹ã«è¿‘ã¥ã„ã¦è©±ã—ã¦ãã ã•ã„ ðŸŽ¤"
            case "es-ES":
                return "Â¡IntentÃ©moslo de nuevo! Por favor, mantÃ©n presionado el micrÃ³fono y habla mÃ¡s cerca del dispositivo ðŸŽ¤"
            case "it-IT":
                return "Riproviamo! Tieni premuto il microfono e parla piÃ¹ vicino al dispositivo ðŸŽ¤"
            case "ko-KR":
                return "ë‹¤ì‹œ ì‹œë„í•´ ë³´ê² ìŠµë‹ˆë‹¤! ë§ˆì´í¬ë¥¼ ê¸¸ê²Œ ëˆ„ë¥´ê³  ê¸°ê¸°ì— ë” ê°€ê¹Œì´ ëŒ€ê³  ë§ì”€í•´ ì£¼ì„¸ìš” ðŸŽ¤"
            case "fr-FR":
                return "Essayons Ã  nouveau ! Maintenez le micro appuyÃ© et parlez plus prÃ¨s de l'appareil ðŸŽ¤"
            case "pt-PT":
                return "Vamos tentar novamente! Mantenha pressionado o microfone e fale mais perto do dispositivo ðŸŽ¤"
            default:
                return "Let's try that again! Please press and hold the microphone and speak a little closer to your device ðŸŽ¤"
            }
        case "recognition_error":
            switch language {
            case "zh-Hans":
                return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰å¬æ¸…æ¥šã€‚è¯·æŒ‰ä½éº¦å…‹é£Žï¼Œè¯´æ…¢ä¸€ç‚¹ï¼Œè¯´å¾—æ›´æ¸…æ™°ä¸€äº› ðŸŽ¤"
            case "ja-JP":
                return "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€èžãå–ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ãƒžã‚¤ã‚¯ã‚’é•·æŠ¼ã—ã—ã¦ã€ã‚‚ã†å°‘ã—ã‚†ã£ãã‚Šã€ã¯ã£ãã‚Šã¨è©±ã—ã¦ãã ã•ã„ ðŸŽ¤"
            case "es-ES":
                return "Lo siento, no pude entender bien. Por favor, mantÃ©n presionado el micrÃ³fono y habla mÃ¡s despacio y claro ðŸŽ¤"
            case "it-IT":
                return "Mi dispiace, non ho capito bene. Per favore, tieni premuto il microfono e parla piÃ¹ lentamente e chiaramente ðŸŽ¤"
            case "ko-KR":
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ìž˜ ì•Œì•„ë“£ì§€ ëª»í–ˆì–´ìš”. ë§ˆì´í¬ë¥¼ ê¸¸ê²Œ ëˆ„ë¥´ê³ , ì¢€ ë” ì²œì²œížˆ, ëª…í™•í•˜ê²Œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”? ðŸŽ¤"
            case "fr-FR":
                return "DÃ©solÃ©, je n'ai pas bien compris. Pourriez-vous maintenir le micro appuyÃ© et parler plus lentement et plus clairement ? ðŸŽ¤"
            case "pt-PT":
                return "Desculpe, nÃ£o entendi bem. Por favor, mantenha pressionado o microfone e fale mais devagar e claramente ðŸŽ¤"
            default:
                return "Sorry, I didn't catch that. Could you please press and hold the microphone and speak more slowly and clearly? ðŸŽ¤"
            }
        default:
            return "An error occurred"
        }
    }

    func startRecording(sourceLanguage: String) {
        isRecording = true
        recognizedText = ""
        errorMessage = nil // Clear previous errors at the start

        // Select the correct recognizer based on sourceLanguage
        let recognizer: SFSpeechRecognizer?
        switch sourceLanguage {
        case "en-US":
            recognizer = speechRecognizerEN
        case "zh-Hans":
            recognizer = speechRecognizerZH
        case "ja-JP":
            recognizer = speechRecognizerJA
        case "es-ES":
            recognizer = speechRecognizerES
        case "it-IT": // Add case for Italian
            recognizer = speechRecognizerIT
        case "ko-KR": // Add case for Korean
            recognizer = speechRecognizerKO
        case "fr-FR": // Add case for French
            recognizer = speechRecognizerFR
        case "pt-PT": // Add case for Portuguese
            recognizer = speechRecognizerPT
        default:
            print("ðŸ”´ Unsupported language code: \(sourceLanguage)")
            recognizer = nil
        }


        guard let selectedRecognizer = recognizer, selectedRecognizer.isAvailable else {
            self.errorMessage = String(format: NSLocalizedString("recognizer_not_available_for_language", 
                comment: "Voice recognition is not available for %@. Please check your internet connection or try another language"), sourceLanguage)
            isRecording = false
            return
        }

        self.recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        guard let request = self.recognitionRequest else {
            self.errorMessage = NSLocalizedString("failed_to_create_recognition_request", 
                comment: "Unable to start voice recognition. Please try again")
            isRecording = false
            return
        }
        // Consider setting requiresOnDeviceRecognition based on availability if needed
        // request.requiresOnDeviceRecognition = selectedRecognizer.supportsOnDeviceRecognition
        request.requiresOnDeviceRecognition = false // Keep as false for now
        request.taskHint = .dictation

        // Add this line to request punctuation
        if #available(iOS 16.0, *) {
            request.addsPunctuation = true
        }

        let session = AVAudioSession.sharedInstance()
        do {
            try session.setCategory(.record, mode: .measurement, options: .duckOthers)
            try session.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            self.errorMessage = String(format: NSLocalizedString("audio_session_config_failed_details", 
                comment: "Unable to access the microphone. Please check your device settings"), error.localizedDescription)
            isRecording = false
            return
        }

        let inputNode = audioEngine.inputNode
        guard inputNode.inputFormat(forBus: 0).channelCount > 0 else {
             self.errorMessage = NSLocalizedString("audio_input_node_unavailable", 
                 comment: "Microphone not found. Please make sure your microphone is connected and working")
             isRecording = false
             return
         }
        let format = inputNode.outputFormat(forBus: 0)
        inputNode.removeTap(onBus: 0)
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: format) { buffer, _ in
            request.append(buffer)
        }

        audioEngine.prepare()
        do {
            try audioEngine.start()
            print("ðŸŽ™ï¸ Audio engine started for \(sourceLanguage)")
        } catch {
            self.errorMessage = String(format: NSLocalizedString("audio_engine_start_failed_details", 
                comment: "Unable to start recording. Please check your microphone connection"), error.localizedDescription)
            isRecording = false
            audioEngine.inputNode.removeTap(onBus: 0)
            return
        }

        // Then update the error handling code in recognitionTask:
        recognitionTask = selectedRecognizer.recognitionTask(with: request) { [weak self] result, error in
            guard let self = self else { return }
    
            if let error = error {
                let nsError = error as NSError
                print("ðŸ”´ Recognition error for \(sourceLanguage): \(error.localizedDescription), Code: \(nsError.code), Domain: \(nsError.domain)")
                DispatchQueue.main.async {
                    if nsError.domain == "kAFAssistantErrorDomain" && nsError.code == 1110 {
                        self.errorMessage = self.getLocalizedErrorMessage(for: "no_speech_detected", language: sourceLanguage)
                    } else {
                        self.errorMessage = self.getLocalizedErrorMessage(for: "recognition_error", language: sourceLanguage)
                    }
                }
                self.audioEngine.stop()
                self.audioEngine.inputNode.removeTap(onBus: 0)
                self.recognitionRequest?.endAudio()
                self.isRecording = false
                self.recognitionTask = nil
                self.recognitionRequest = nil
                return
            }
    
            guard let result = result else { return }
    
            DispatchQueue.main.async {
                let newText = result.bestTranscription.formattedString
                if self.recognizedText != newText {
                    self.recognizedText = newText
                    print("ðŸ‘‚ Recognized (\(sourceLanguage)): \(self.recognizedText)")
                }
                self.errorMessage = nil // Clear error on successful partial or final result
    
                if result.isFinal {
                    print("âœ… Final recognition result (\(sourceLanguage)): \(self.recognizedText)")
                }
            }
        }
    }

    func stopRecording() {
        if audioEngine.isRunning {
            audioEngine.stop()
            audioEngine.inputNode.removeTap(onBus: 0)
            recognitionRequest?.endAudio()
            print("ðŸ›‘ Audio engine stopped.")
        } else {
            print("âš ï¸ Audio engine was not running.")
        }

        if recognitionTask != nil {
            recognitionTask?.finish()
            recognitionTask = nil
            print("ðŸ Recognition task finished.")
        } else {
            print("âš ï¸ Recognition task was already nil.")
        }
        
        recognitionRequest = nil

        if isRecording {
            DispatchQueue.main.async {
                self.isRecording = false
                print("ðŸŽ™ Recording stopped state updated. Final text: \(self.recognizedText)")
            }
        }
    }

    func checkAndRequestPermission() async -> Bool {
        let status = SFSpeechRecognizer.authorizationStatus()

        switch status {
        case .authorized:
            return true
        case .notDetermined:
            return await withCheckedContinuation { continuation in
                SFSpeechRecognizer.requestAuthorization { status in
                    continuation.resume(returning: status == .authorized)
                }
            }
        case .denied, .restricted:
            DispatchQueue.main.async {
                 self.errorMessage = NSLocalizedString("speech_permission_denied_or_restricted_check_settings", 
                     comment: "Voice recognition is disabled. Please enable it in your device Settings")
             }
            return false
        @unknown default:
            DispatchQueue.main.async {
                self.errorMessage = NSLocalizedString("unknown_speech_permission_status", 
                    comment: "Unable to determine voice recognition permissions. Please check your device Settings")
            }
            return false
        }
    }
}
